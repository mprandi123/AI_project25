{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Alberto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "import logging\n",
    "import missingno\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 4\n",
    "N = 50  # combinations\n",
    "rnd_seed = 42\n",
    "date_start = '2020-01-30'\n",
    "\n",
    "# Select metric for evaluation 1\\2\\3 (r2\\mse\\mae)\n",
    "metric_choice = '1' # Must be in ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \n",
    "    #Plots and prints the top 10 most important features.\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(10)\n",
    "\n",
    "\n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(feature_importance_df[\"Feature\"], feature_importance_df[\"Importance\"], color=\"steelblue\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(f\"Feature Importance - {model_name}\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Sharpe Ratio calculation\n",
    "def rolling_sharpe_ratio(returns, window=30, risk_free_rate=0.00007858): # Daily risk-free considering 0.02% annual rate\n",
    "\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    rolling_mean = pd.Series(excess_returns).rolling(window).mean()\n",
    "    rolling_std = pd.Series(excess_returns).rolling(window).std()\n",
    "    sharpe_ratio = rolling_mean / rolling_std\n",
    "\n",
    "    # Replace infinities with 0\n",
    "    sharpe_ratio.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    # Ensure no NaN values\n",
    "    sharpe_ratio.fillna(0, inplace=True)\n",
    "\n",
    "    return sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation function\n",
    "\n",
    "def time_grouped_cv(model, X, y):\n",
    "    \n",
    "    \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=False)\n",
    "    scores = []\n",
    "    \n",
    "    combined_matrix_filtered = combined_matrix.loc[X.index].copy()\n",
    "    combined_matrix_filtered['time_step'] = combined_matrix_filtered['time_step'].reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nStarting Time-Grouped Cross-Validation...\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(np.unique(combined_matrix_filtered['time_step'])), start=1):\n",
    "        '''\n",
    "        log_and_print(f\"Processing Fold {fold}/5...\")'\n",
    "        '''\n",
    "        time.sleep(0.2)  # Just to make it more readable if running fast\n",
    "        \n",
    "        train_steps = combined_matrix_filtered['time_step'].iloc[train_idx].values\n",
    "        test_steps = combined_matrix_filtered['time_step'].iloc[test_idx].values\n",
    "        \n",
    "        train_mask = combined_matrix_filtered['time_step'].isin(train_steps).values\n",
    "        test_mask = combined_matrix_filtered['time_step'].isin(test_steps).values       \n",
    "        \n",
    "        X_train, X_test = X.loc[train_mask], X.loc[test_mask]\n",
    "        y_train, y_test = y.loc[train_mask], y.loc[test_mask]        \n",
    "                \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate selected metric\n",
    "        score = selected_metric_func(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        \n",
    "    mean_score = np.mean(scores)\n",
    "\n",
    "    return mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the chosen metric the function evaluates the model\n",
    "def evaluate_model(name, model, X_test, y_test, metric):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "\n",
    "    # Calculate Directional Accuracy\n",
    "    y_pred_dir = np.sign(y_pred)\n",
    "    y_test_dir = np.sign(y_test)\n",
    "    directional_accuracy = np.mean(y_pred_dir == y_test_dir)\n",
    "\n",
    "\n",
    "    # Return selected metric score\n",
    "    if metric == \"RÂ²\":\n",
    "        return r2\n",
    "    elif metric == \"MAE\":\n",
    "        return mae\n",
    "    elif metric == \"DA\":\n",
    "        return directional_accuracy\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid metric selected: {metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names, model_name):\n",
    "    \n",
    "    \"\"\"Plots and prints the top 10 most important features.\"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(10)\n",
    "\n",
    "\n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(feature_importance_df[\"Feature\"], feature_importance_df[\"Importance\"], color=\"steelblue\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(f\"Feature Importance - {model_name}\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculates the percentage of times the predicted and actual returns have the same sign.\"\"\"\n",
    "    correct_directions = (np.sign(y_true) == np.sign(y_pred)).sum()\n",
    "    return correct_directions / len(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_random_search(model, param_grid, X, y, n_iter=10):\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    feature_columns = X.columns.tolist()  # Get the feature names directly from X\n",
    "        \n",
    "\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        params = {k: np.random.choice(v) for k, v in param_grid.items()}\n",
    "        model.set_params(**params)\n",
    "        \n",
    "        score = time_grouped_cv(model, X, y)\n",
    "\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "    \n",
    "    \n",
    "    model.set_params(**best_params)    \n",
    "    model.fit(X, y)\n",
    "    # Feature Importance Extraction\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        plot_feature_importances(model, feature_columns, model.__class__.__name__)\n",
    "    return model, best_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculates the percentage of times the predicted and actual returns have the same sign.\"\"\"\n",
    "    correct_directions = (np.sign(y_true) == np.sign(y_pred)).sum()\n",
    "    return correct_directions / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(df):\n",
    "   \n",
    "    \n",
    "    # Inflation differential\n",
    "    df[\"inflation_diff\"] = df[\"INFLEUR5Y\"] - df[\"INFLUSD5Y\"]\n",
    "    \n",
    "    # Volatility spread\n",
    "    df[\"VIX_V2X_spread\"] = df[\"VIX\"] - df[\"V2X\"]\n",
    "    \n",
    "    # Yield spreads\n",
    "    df[\"EUR_yield_spread\"] = df[\"5Y EUR RATE\"] - df[\"3M EUR RATE\"]\n",
    "    df[\"USD_yield_spread\"] = df[\"5Y USD RATE\"] - df[\"3M USD RATE\"]\n",
    "    \n",
    "    # FX spread\n",
    "    df[\"EURUSD_EURJPY_spread\"] = df[\"EURUSD\"] - df[\"EURJPY\"]\n",
    "    \n",
    "    # Commodity spread\n",
    "    df[\"GOLD_WTI_spread\"] = df[\"GOLD\"] - df[\"WTI\"]\n",
    "    \n",
    "    # Differenced features\n",
    "    for col in [\"VIX\", \"V2X\", \"MOVE\", \"EURUSD\", \"EURJPY\", \"GOLD\", \"5Y EUR RATE\", \"inflation_diff\"]:\n",
    "        df[f\"{col}_diff\"] = df[col].diff()\n",
    "    \n",
    "    # Interaction terms\n",
    "    df[\"VIX_MOVE_interaction\"] = df[\"VIX_diff\"] * df[\"MOVE_diff\"]\n",
    "    df[\"VIX_EURUSD_interaction\"] = df[\"VIX_diff\"] * df[\"EURUSD_diff\"]\n",
    "    \n",
    "    # Momentum Indicators: Rolling Averages (7-day and 30-day)\n",
    "    for col in df.columns:\n",
    "        if col.endswith('_diff'):\n",
    "            df[f'{col}_ma7'] = df[col].rolling(window=7).mean()\n",
    "            df[f'{col}_ma30'] = df[col].rolling(window=30).mean()\n",
    "\n",
    "    # Volatility Clustering: Rolling Standard Deviations\n",
    "    for col in ['VIX', 'V2X', 'MOVE']:\n",
    "        df[f'{col}_volatility_7d'] = df[col].rolling(window=7).std()\n",
    "        df[f'{col}_volatility_30d'] = df[col].rolling(window=30).std()\n",
    "\n",
    "    # Non-linear Effects: Squared Terms\n",
    "    for col in df.columns:\n",
    "        if col.endswith('_diff'):\n",
    "            df[f'{col}_squared'] = df[col] ** 2\n",
    "\n",
    "    # Signal Ratios\n",
    "    df['VIX_MOVE_ratio'] = df['VIX'] / df['MOVE'].replace(0, np.nan)\n",
    "    df['EURUSD_VIX_ratio'] = df['EURUSD'] / df['VIX'].replace(0, np.nan)\n",
    "    df['EUR_Inflation_Rate_ratio'] = df['5Y EUR RATE'] / df['INFLUSD5Y'].replace(0, np.nan)\n",
    "    df['USD_Rate_Spread_ratio'] = df['5Y USD RATE'] / df['3M USD RATE'].replace(0, np.nan)\n",
    "    \n",
    "    # Drop NaN rows after diff()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids\n",
    "\n",
    "gbr_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'min_samples_split': [2, 5, 10, 20, 30],    # Added regularization\n",
    "    'min_samples_leaf': [1, 2, 4]               # Added regularization\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5],              # Added regularization\n",
    "    'gamma': [0, 0.1, 0.2],                     # Added regularization\n",
    "    'reg_alpha': [0, 0.1, 0.5],                 # L1 regularization (lasso)\n",
    "    'reg_lambda': [1, 1.5, 2.0, 3.0]            # L2 regularization (ridge)\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],           # Added regularization\n",
    "    'max_samples': [0.5, 0.75, 1.0]             # Added regularization\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map choice to metric\n",
    "metric_map = {\n",
    "    \"1\": (\"RÂ²\", r2_score),\n",
    "    \"2\": (\"MAE\", mean_absolute_error),\n",
    "    \"3\": (\"DA\", directional_accuracy)\n",
    "}\n",
    "\n",
    "# Get selected metric\n",
    "if metric_choice not in metric_map:\n",
    "    print()\n",
    "    print(\"Invalid choice. Defaulting to RÂ².\")\n",
    "    selected_metric_name, selected_metric_func = metric_map[\"1\"]\n",
    "else:\n",
    "    selected_metric_name, selected_metric_func = metric_map[metric_choice]\n",
    "print()\n",
    "print(f\"Selected metric: {selected_metric_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data\n",
    "filename = 'IP25_DATA_day1/CPO_DATA.xlsx'\n",
    "data = pd.read_excel(filename, header=3, sheet_name='RAW_DATA')\n",
    "data.set_index('date', inplace=True)\n",
    "data = data[date_start:]\n",
    "\n",
    "missingno.matrix(data.iloc[:,0:4],figsize=(5,3), fontsize=12)\n",
    "plt.title(\"ASSETS' RETURNS\")\n",
    "plt.show()\n",
    "\n",
    "missingno.matrix(data.iloc[:,4:],figsize=(5,3), fontsize=12)\n",
    "plt.title(\"MARKET FACTORS\")\n",
    "plt.show()\n",
    "\n",
    "data = add_engineered_features(data)\n",
    "\n",
    "missingno.matrix(data,figsize=(5,3), fontsize=12)\n",
    "plt.title(\"FULL FEATURES\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns for the first 4 columns (assets) and drop NaN rows\n",
    "data.iloc[:, 0:n_assets] = data.iloc[:, 0:n_assets].pct_change()\n",
    "data = data.dropna()\n",
    "\n",
    "# Extract asset returns (4 assets) and features (15 features)\n",
    "asset_returns = data.iloc[:, 0:n_assets]\n",
    "features = data.iloc[:, n_assets:]\n",
    "\n",
    "# Generate random weight combinations using Dirichlet distribution\n",
    "np.random.seed(rnd_seed)\n",
    "random_weights = np.random.dirichlet(np.ones(n_assets), size=N)\n",
    "\n",
    "# Convert to DataFrame\n",
    "portfolio_weights = pd.DataFrame(random_weights, columns=['w1', 'w2', 'w3', 'w4'])\n",
    "\n",
    "print(portfolio_weights.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps (T) and portfolios (N)\n",
    "T = len(features)\n",
    "N = len(portfolio_weights)\n",
    "\n",
    "\n",
    "### Weights-Features matrix ###\n",
    "# Create the correct NxT matrix\n",
    "features_repeated = features.loc[features.index.repeat(N)].reset_index(drop=True) # Repeat each row of features N times\n",
    "portfolio_repeated = pd.DataFrame(   # Tile (cycle) portfolio weights T times so they align with features\n",
    "    np.tile(portfolio_weights, (T, 1)),\n",
    "    columns=['w1', 'w2', 'w3', 'w4'])\n",
    "combined_matrix = pd.concat([portfolio_repeated, features_repeated], axis=1) # Combine features and weights side by side\n",
    "\n",
    "\n",
    "### Adding PTF returns ###\n",
    "# Recalculate portfolio returns using weights and asset returns\n",
    "portfolio_returns = np.sum(portfolio_repeated.values * asset_returns.loc[asset_returns.index.repeat(N)].values, axis=1)\n",
    "portfolio_returns_df = pd.DataFrame(portfolio_returns, columns=['portfolio_return'])  # Convert to DataFrame for consistency\n",
    "combined_matrix['portfolio_return'] = portfolio_returns_df['portfolio_return'].values # Add portfolio returns to the combined matrix\n",
    "combined_matrix['portfolio_return'] = portfolio_returns_df['portfolio_return'].values # Add portfolio returns to the combined matrix\n",
    "\n",
    "\n",
    "### Adding Sharpe Ratios ###\n",
    "combined_matrix['sharpe_ratio'] = rolling_sharpe_ratio(combined_matrix['portfolio_return'], window=30) # Calculate Sharpe ratios for all rows (NxT)\n",
    "combined_matrix['sharpe_ratio'].fillna(0, inplace=True) # Handle NaNs by replacing with zero \n",
    "combined_matrix = combined_matrix[combined_matrix['sharpe_ratio'] != 0].reset_index(drop=True) # Remove rows where Sharpe ratio is NaN or zero\n",
    "\n",
    "\n",
    "# Group rows by time steps\n",
    "time_steps = np.arange(len(features))\n",
    "combined_matrix['time_step'] = np.repeat(time_steps, N)[:len(combined_matrix)] # Assign time step groups, repeated N times (so rows for t1-tN are grouped)\n",
    "\n",
    "\n",
    "### Checks ###\n",
    "missingno.matrix(combined_matrix,figsize=(5,3), fontsize=12)\n",
    "plt.title(\"COMBINED MATRIX\")\n",
    "plt.show()\n",
    "\n",
    "# Now the combined matrix has the following structure: weights, features, portfolio returns, sharpe ratio, time steps (0x50-Nx50)\n",
    "display(combined_matrix.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION MATRIX\n",
    "N_P = 2 # how many time step to predict from bottom\n",
    "predict_matrix = combined_matrix[(-N*N_P):]\n",
    "combined_matrix = combined_matrix[0:(-N*N_P)]\n",
    "\n",
    "\n",
    "### Checks ###\n",
    "missingno.matrix(combined_matrix,figsize=(5,3), fontsize=12)\n",
    "plt.title(\"COMBINED MATRIX\")\n",
    "plt.show()\n",
    "\n",
    "missingno.matrix(predict_matrix,figsize=(5,3), fontsize=12)\n",
    "plt.title(\"PREDICTION MATRIX\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = combined_matrix.drop(columns=['sharpe_ratio', 'portfolio_return', 'time_step'])\n",
    "y = combined_matrix['sharpe_ratio']\n",
    "\n",
    "# Ensure indices of X and y match\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Matching indices: {(X.index == y.index).all()}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_idx = int(len(X) * 0.7)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(random_state=rnd_seed)\n",
    "rf = RandomForestRegressor(random_state=rnd_seed)\n",
    "xgb = XGBRegressor(random_state=rnd_seed)\n",
    "\n",
    "# Random search with time grouping\n",
    "rf_best_model, rf_best_score, rf_best_params = custom_random_search(rf, rf_params, X_train, y_train)\n",
    "gbr_best_model, gbr_best_score, gbr_best_params = custom_random_search(gbr, gbr_params, X_train, y_train)\n",
    "xgb_best_model, xgb_best_score, xgb_best_params = custom_random_search(xgb, xgb_params, X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"\\nBest Random Forest hyperparameters:\", rf_best_params)\n",
    "print(\"Regularization params (RF): max_features={}, max_samples={}\".format(\n",
    "    rf_best_params.get('max_features'), rf_best_params.get('max_samples')))\n",
    "\n",
    "print(\"\\nBest Gradient Boosting hyperparameters:\", gbr_best_params)\n",
    "print(\"Regularization params (GBR): min_samples_split={}, min_samples_leaf={}\".format(\n",
    "    gbr_best_params.get('min_samples_split'), gbr_best_params.get('min_samples_leaf')))\n",
    "\n",
    "print(\"\\nBest XGBoost hyperparameters:\", xgb_best_params)\n",
    "print(\"Regularization params (XGB): min_child_weight={}, gamma={}, reg_alpha={}, reg_lambda={}\".format(\n",
    "    xgb_best_params.get('min_child_weight'), \n",
    "    xgb_best_params.get('gamma'),\n",
    "    xgb_best_params.get('reg_alpha'),\n",
    "    xgb_best_params.get('reg_lambda')))\n",
    "\n",
    "_ = evaluate_model(\"Random Forest\", rf_best_model, X_test, y_test,\"MAE\")\n",
    "_ = evaluate_model(\"Gradient Boosting\", gbr_best_model, X_test, y_test,\"MAE\")\n",
    "_ = evaluate_model(\"XGBoost\", xgb_best_model, X_test, y_test,\"MAE\")\n",
    "\n",
    "\n",
    "# prediction\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "X_pred = predict_matrix.drop(columns=['sharpe_ratio', 'portfolio_return', 'time_step'])\n",
    "\n",
    "yhat_rf = rf_best_model.predict(X_pred)\n",
    "yhat_gbr = gbr_best_model.predict(X_pred)\n",
    "yhat_xgb = xgb_best_model.predict(X_pred)\n",
    "\n",
    "X_pred['yhat_rf'] = yhat_rf\n",
    "X_pred['yhat_gbr'] = yhat_gbr\n",
    "X_pred['yhat_xgb'] = yhat_xgb\n",
    "\n",
    "W_rf = X_pred[X_pred['yhat_rf'] == X_pred['yhat_rf'].max()].iloc[:,0:4]\n",
    "W_gbr = X_pred[X_pred['yhat_gbr'] == X_pred['yhat_gbr'].max()].iloc[:,0:4]\n",
    "W_xgb = X_pred[X_pred['yhat_xgb'] == X_pred['yhat_xgb'].max()].iloc[:,0:4]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IP_AI_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
